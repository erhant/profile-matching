{"bio": "Dad, Teacher, Computer Scientist, Unreasonable Optimist", "born": "", "followers": "1,743 Followers", "following": "355 Following", "handle": "@UdacityDave", "joined": "Joined February 2012", "location": "Charlottesville, VA", "name": "David Evans", "photo": "https://pbs.twimg.com/profile_images/1259100977782718465/L-YI3PQc_400x400.png", "site": "cs.virginia.edu/evans", "tweets": ["Happy I (and @danwallach) was able to help out this one:PolitiFact - Fact-checking Donald Trump\u2019s tweet firing Christopher KrebsPolitiFact is a fact-checking website that rates the accuracy of claims by elected officials and others on its Truth-O-Meter.politifact.com1", "Yes, totally agree - once we understand a domain well enough to build a perfect robust classifier, we won't need ML anymore.Quote TweetSiddharth Garg @sg1753 \u00b7 Nov 18Replying to @UdacityDave100% --- but then we obviously can't protect against all possible invariants with ML, because that will just be a complete specification (and if we had one, why ML?). So, do we basically end up calling any malware mod. that retains bad functionality but evades ML adversarial?3", "Maybe - but for malware, the invariants don't need to be subjective - they can be defined based on preserving the intended malicious behavior when they are executed. For human perception, we need artificial invariants or be willing to test with actual humans.Quote TweetSiddharth Garg @sg1753 \u00b7 Nov 18Replying to @sg1753 and @UdacityDaveSo I guess what I'm saying, rather inartfully perhaps, is that these \"reasonable invariants\" are subjective, right? And ultimately subjective to human expectations...even for malware classifiers.1", "Great to see progress towards meaningfully robust malware classifiers!Quote TweetYizheng Chen@surrealyz \u00b7 Nov 18Our models, code to train 19 models, 7 attacks w/ adaptive ones, for USENIX Security paper \"On Training Robust PDF Malware Classifiers\" https://bit.ly/32TLykC We couldn't fully evade certifiably Robust A+B+E using adaptive strategies over EvadeML. We hope other researchers can.Show this thread16", "There are lots of domains (e.g., malware classification) where a ground truth isn't based on human perception. And, humans are that good at many classification problems (and often disagree, so determining \"ground truth\" based on human perception is difficult).Quote TweetSiddharth Garg @sg1753 \u00b7 Nov 18Replying to @florian_tramer @pinyuchenTW and 3 othersIn class, I always define an adversarial perturbation as a perturbation that \"fools\" a model but not a human. Ultimately, our ground truth has always been human classifiers, its just that we don't have a model for a human.21"]}