{"bio": "", "born": "", "followers": "", "following": "", "handle": "", "joined": "", "location": "", "name": "", "photo": null, "site": "", "tweets": ["Expert annotators or crowdsourcing? At least for our domain/task, a fifth of the data annotated by a single expert results in better token-level F1 for a sequence labeling task (finding medical interventions) than all the crowdsourced dataQuote Tweetbyron wallace@byron_c_wallace \u00b7 May 21, 2019crowdsourced annotations in specialized domains can be noisy, but domain experts are $$$ -- so we learn to *predict* which instances will be difficult, and route these to the experts: http://bit.ly/2JUs1b1 (#NAACL2019 w/@yinfeiy, @oshinagarwal92, chris tar, @ani_nenkova)4950", "", "If a field went through the trouble of changing the acronym of their flagship conference, I\u2019d use the new acronym when talking about the venue, even for editions before the change7", "2) Best Paper: Are Some Words Worth More than Others? by Shiran Dudy and Steven Bedrick; 3) Best Paper: Fill in the BLANC: Human-free quality estimation of document summaries by Oleg Vasilyev, Vedant Dharnidharka and John Bohannon. 2/2124", "Big congrats to the winners of the best paper awards: 1) Best Paper: Improving Text Generation Evaluation with Batch Centering and Tempered Word Mover Distance by Xi Chen, Nan Ding, Tomer Levinboim and Radu Soricut; 1/21615"]}