{"bio": "", "born": "", "followers": "", "following": "", "handle": "", "joined": "", "location": "", "name": "", "photo": null, "site": "", "tweets": ["Wow! Does this mean we will have negative productivity in 2021?!Quote TweetTamay@tamaybes \u00b7 Nov 22I found that the marginal returns of researchers are rapidly declining. There is what\u2019s called a \u201cstanding on toes\u201d effect: researcher productivity declines as the field grows. Because ML has recently grown very quickly, this makes better ML models much harder to find.Show this thread2", "Training on test points is a no-no. But training on test points with fake labels turns out to be a good idea. Aurick's blog post on amortized conditional normalized likelihood (ACNML) shows how NML provides a principled view of \"training on test points\": https://bair.berkeley.edu/blog/2020/11/16/acnml/\u2026325129", "Curve approximation using Fourier series corresponds to drawing the curve using entangled ellipses. https://en.wikipedia.org/wiki/Fourier_series\u2026154346", "This is somewhat close to reality!Quote TweetMoshe Vardi@vardi \u00b7 Nov 17Your Thesis Committee :-)14", ""]}