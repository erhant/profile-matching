{"bio": "", "born": "", "followers": "", "following": "", "handle": "", "joined": "", "location": "", "name": "", "photo": null, "site": "", "tweets": ["Can Temporal-Difference and Q-Learning Learn Representation? (https://arxiv.org/abs/2006.04761)\n\nIs linearity/linearization necessary for the convergence of TD/Q-learning? We show linearity wrt finite-dim parameters is not \u2014 linearity wrt infinite-dim distributions comes to the rescue.232", "", "Will give a talk on GAN global landscape at \u201cdeep learning and low dimensional model workshop\u201d in a hour, organized by Yi Ma, Qing Qu, Zhuhai Zhu, etc. Fully online this year.329", "I will talk about this result tomorrow @RLtheory virtual seminar: https://sites.google.com/view/rltheoryseminars/next-seminar\u2026Quote TweetSimon Shaolei Du@SimonShaoleiDu \u00b7 Sep 29Reinforcement Learning is a strict generalization of bandits due to long planning horizon and unknown transition, but is RL fundamentally harder? Our new algorithm for tabular RL, Monotonic Value Propogation (MVP), approaches the bandits' lower bound!\nhttps://arxiv.org/abs/2009.13503Show this thread435", ""]}