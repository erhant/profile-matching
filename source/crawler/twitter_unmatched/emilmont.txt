{"bio": "", "born": "", "followers": "", "following": "", "handle": "", "joined": "", "location": "", "name": "", "photo": null, "site": "", "tweets": ["Our paper is out: \"Don't Parse, Generate! A Sequence to Sequence Architecture for Task-Oriented Semantic Parsing\" - https://amazon.science/publications/dont-parse-generate-a-sequence-to-sequence-architecture-for-task-oriented-semantic-parsing\u202616", "", "It was a beautiful spring day #SelfIsolation #springbreak2020 #backyard #backgarden #illustration418", "We're starting 2020 with a new digital platform for our scientific community at Amazon. Follow this account to learn more! #AmazonScience #NewYear37", "A new era of NLP has just begun a few days ago: large pretraining models (Transformer 24 layers, 1024 dim, 16 heads) + massive compute is all you need. BERT from @GoogleAI: SOTA results on everything https://arxiv.org/abs/1810.04805. Results on SQuAD are just mind-blowing. Fun time ahead!135061K"]}